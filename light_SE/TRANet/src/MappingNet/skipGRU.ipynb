{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6448fc2-4078-408d-9ecf-0ac78217778a",
   "metadata": {},
   "source": [
    "# Skip RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce7bf3a6-1528-4cc8-9de1-dc7d4688428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import xavier_normal_\n",
    "from torch.autograd import Function\n",
    "\n",
    "    \n",
    "class STEFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(cls,x):\n",
    "        return x.round()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cls,grad):\n",
    "        return grad\n",
    "\n",
    "\n",
    "class STELayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STELayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        binarizer = STEFunction.apply\n",
    "        return binarizer(x)\n",
    "\n",
    "\n",
    "class SkipGRUCell(nn.Module):\n",
    "    def __init__(self, ic, hc):\n",
    "        super(SkipGRUCell, self).__init__()\n",
    "        self.ste = STELayer()\n",
    "        self.cell = nn.GRUCell(ic, hc)\n",
    "        self.linear = nn.Linear(hc, 1)\n",
    "\n",
    "        xavier_normal_(self.linear.weight)\n",
    "        self.linear.bias.data.fill_(1)\n",
    "\n",
    "    def forward(self, x, u, h, skip=False, delta_u=None):\n",
    "        # x: (bs, ic)\n",
    "        # u: (bs, 1)\n",
    "        # h: (bs, hc)\n",
    "        # skip: [False or True] * bs\n",
    "        # delta_u: [skip=True -> (1) / skip=False -> None] * bs\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        binarized_u = self.ste(u)                # (bs, 1)\n",
    "\n",
    "        skip_idx = [i for i, cur_skip in enumerate(skip) if cur_skip]\n",
    "        skip_num = len(skip_idx)\n",
    "        no_skip = [not cur_skip for cur_skip in skip]\n",
    "\n",
    "        if skip_num > 0:\n",
    "            # (skip_num, ic), (skip_num, 1), (skip_num, hc)\n",
    "            x_s, u_s, h_s = x[skip], u[skip], h[skip]\n",
    "            binarized_u_s = binarized_u[skip]        # (skip_num, 1)\n",
    "\n",
    "            # (skip_num, 1)\n",
    "            delta_u_s = [cur_delta_u for cur_skip,\n",
    "                         cur_delta_u in zip(skip, delta_u) if cur_skip]\n",
    "            delta_u_s = torch.stack(delta_u_s)\n",
    "\n",
    "            # computing skipped parts\n",
    "            new_h_s = h_s * (1 - binarized_u_s)        # (skip_num, hc)\n",
    "            new_u_s = torch.clamp(u_s + delta_u_s, 0, 1) * \\\n",
    "                (1 - binarized_u_s)  # (skip_num, 1)\n",
    "\n",
    "        if skip_num < bs:\n",
    "            # (bs-skip_num, ic), (bs-skip_num, 1), (bs-skip_num, hc)\n",
    "            x_n, u_n, h_n = x[no_skip], u[no_skip], h[no_skip]\n",
    "            binarized_u_n = binarized_u[no_skip]  # (bs-skip_num, 1)\n",
    "\n",
    "            # computing non-skipped parts\n",
    "            new_h_n = self.cell(x_n, h_n)  # (bs-skip_num, hc)\n",
    "            new_h_n = new_h_n * binarized_u_n            # (bs-skip_num, hc)\n",
    "            delta_u_n = torch.sigmoid(self.linear(new_h_n))        # (bs-skip_num, 1)\n",
    "            new_u_n = delta_u_n * binarized_u_n                    # (bs-skip_num, 1)\n",
    "\n",
    "        # merging skipped and non-skipped parts back\n",
    "        if 0 < skip_num < bs:\n",
    "            idx = torch.full((bs,), -1, dtype=torch.long)\n",
    "            idx[skip_idx] = torch.arange(0, len(skip_idx), dtype=torch.long)\n",
    "            idx[idx==-1] = torch.arange(len(skip_idx), bs, dtype=torch.long)\n",
    "\n",
    "            new_u = torch.cat([new_u_s, new_u_n], 0)[idx]        # (bs, 1)\n",
    "            new_h = torch.cat([new_h_s, new_h_n], 0)[idx]        # (bs, hc)\n",
    "            delta_u = torch.cat([delta_u_s, delta_u_n], 0)[idx]    # (bs, 1)\n",
    "\n",
    "        # no need to merge when skip doesn't exist\n",
    "        elif skip_num == 0:\n",
    "            new_u = new_u_n\n",
    "            new_h = new_h_n\n",
    "            delta_u = delta_u_n\n",
    "\n",
    "        # no need to merge when everything is skip\n",
    "        elif skip_num == bs:\n",
    "            new_u = new_u_s\n",
    "            new_h = new_h_s\n",
    "            delta_u = delta_u_s\n",
    "\n",
    "        n_skips_after = (0.5 / new_u).ceil() - 1  # (bs, 1)\n",
    "        return binarized_u, new_u, (new_h,), delta_u, n_skips_after\n",
    "\n",
    "\n",
    "class SkipGRUCellNoSkip(nn.Module):\n",
    "    def __init__(self, ic, hc):\n",
    "        super(SkipGRUCellNoSkip, self).__init__()\n",
    "        self.ste = STELayer()\n",
    "        self.cell = nn.GRUCell(ic, hc)\n",
    "        self.linear = nn.Linear(hc, 1)\n",
    "        \n",
    "        xavier_normal_(self.linear.weight)\n",
    "        self.linear.bias.data.fill_(1)\n",
    "\n",
    "    def forward(self, x, u, h):\n",
    "        # x: (bs, ic)\n",
    "        # u: (bs, 1)\n",
    "        # h: (bs, hc)\n",
    "\n",
    "        # computing the states\n",
    "        binarized_u = self.ste(u)                # (bs, 1)\n",
    "        new_h = self.cell(x, h)  # (bs, hc)\n",
    "        new_h = new_h * binarized_u + (1 - binarized_u) * h      # (bs, hc)\n",
    "        delta_u = torch.sigmoid(self.linear(new_h))        # (bs, 1)\n",
    "        new_u = delta_u * binarized_u + \\\n",
    "            torch.clamp(u + delta_u, 0, 1) * (1 - binarized_u)  # (bs, 1)\n",
    "\n",
    "        return binarized_u, new_u, new_h, delta_u\n",
    "\n",
    "class SkipGRU(nn.Module):\n",
    "    def __init__(self, ic, hc, layer_num=2, return_total_u=False, learn_init=False, no_skip=False,batch_first = False):\n",
    "        super(SkipGRU, self).__init__()\n",
    "        self.ic = ic\n",
    "        self.hc = hc\n",
    "        self.layer_num = layer_num\n",
    "        self.return_total_u = return_total_u\n",
    "        self.no_skip = no_skip\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        if no_skip:\n",
    "            cur_cell = SkipGRUCellNoSkip\n",
    "        else:\n",
    "            cur_cell = SkipGRUCell\n",
    "\n",
    "        self.cells = nn.ModuleList([cur_cell(ic, hc)])\n",
    "        for _ in range(self.layer_num - 1):\n",
    "            cell = cur_cell(hc, hc)\n",
    "            self.cells.append(cell)\n",
    "\n",
    "        self.hiddens = self.init_hiddens(learn_init)\n",
    "        print(\"hidden : {}\".format(self.hiddens.shape))\n",
    "\n",
    "    def init_hiddens(self, learn_init):\n",
    "        if learn_init:\n",
    "            h = nn.Parameter(torch.randn(self.layer_num, 1, self.hc))\n",
    "        else:\n",
    "            h = nn.Parameter(torch.zeros(self.layer_num, 1, self.hc), requires_grad=False)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x, hiddens=None):\n",
    "        device = x.device\n",
    "        if self.batch_first : \n",
    "            x = torch.permute(x,(1,0,2))\n",
    "        \n",
    "        x_len, bs, _ = x.shape    # (x_len, bs, ic)\n",
    "\n",
    "        if hiddens is None:\n",
    "            h = self.hiddens\n",
    "            h = h.repeat(1, bs, 1)\n",
    "        else:\n",
    "            h = hiddens\n",
    "        u = torch.ones(self.layer_num, bs, 1).to(device)            # (l, bs, 1)\n",
    "\n",
    "        hs = []\n",
    "        lstm_input = x             # (x_len, bs, ic)\n",
    "\n",
    "        skip = [False] * bs\n",
    "        delta_u = [None] * bs\n",
    "\n",
    "        binarized_us = []\n",
    "\n",
    "        for i in range(self.layer_num):\n",
    "            cur_hs = []\n",
    "            cur_h = h[i].unsqueeze(0)  # (1, bs, hc)\n",
    "            cur_u = u[i]               # (bs, 1)\n",
    "\n",
    "            for j in range(x_len):\n",
    "                if self.no_skip:\n",
    "                    # (bs, 1), ((bs, hc), (bs, hc)), (bs, 1), (bs, 1)\n",
    "                    binarized_u, cur_u, cur_h, delta_u = self.cells[i](\n",
    "                        lstm_input[j], cur_u, cur_h[0])\n",
    "                    binarized_us.append(binarized_u)\n",
    "                else:\n",
    "                    # (bs, 1), ((bs, hc), (bs, hc)), (bs, 1), (bs, 1)\n",
    "                    binarized_u, cur_u, cur_h, delta_u, n_skips_after = self.cells[i](\n",
    "                        lstm_input[j], cur_u, cur_h[0], skip, delta_u)\n",
    "                    binarized_us.append(binarized_u)\n",
    "                    skip = (n_skips_after[:, 0] > 0).tolist()\n",
    "\n",
    "                # (1, bs, hc) / (1, bs, hc)\n",
    "                cur_h = cur_h[0].unsqueeze(0)\n",
    "                cur_hs.append(cur_h)\n",
    "\n",
    "            # (x_len, bs, hc)\n",
    "            lstm_input = torch.cat(cur_hs, dim=0)\n",
    "            hs.append(cur_h)\n",
    "\n",
    "        # (bs, seq * layer_num)\n",
    "        total_u = torch.cat(binarized_us, 1)\n",
    "        # (x_len, bs, hc)\n",
    "        out = lstm_input\n",
    "        # (l, bs, hc)\n",
    "        hs = torch.cat(hs, dim=0)\n",
    "        \n",
    "        if self.batch_first : \n",
    "            out = torch.permute(out,(1,0,2))\n",
    "\n",
    "        if self.return_total_u:\n",
    "            return out, (hs,), total_u\n",
    "        return out, (hs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50de9d1e-f4fa-405b-baa7-22bd93d3d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden : torch.Size([2, 1, 256])\n",
      "torch.Size([64, 40, 256])\n",
      "torch.Size([64, 40, 256])\n",
      "torch.Size([2, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "ic = 256\n",
    "hc = 256\n",
    "B = 64\n",
    "L = 40\n",
    "\n",
    "h = torch.rand(2,B,ic)\n",
    "\n",
    "m1 = SkipGRU(ic, hc, layer_num=2, learn_init=False, no_skip=False,batch_first=True)\n",
    "\n",
    "x = torch.rand(B,L,ic)\n",
    "print(x.shape)\n",
    "\n",
    "y = m1(x,h)\n",
    "print(y[0].shape)\n",
    "print(y[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7c5e01-74c4-4bc4-8b2b-42c955d3e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 19.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05060796737670899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 820.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014890909194946289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "ic = 256\n",
    "hc = 256\n",
    "B = 64\n",
    "L = 40\n",
    "\n",
    "m1 = SkipGRU(ic, hc, layer_num=2, return_total_u=True, learn_init=False, no_skip=False,batch_first=True)\n",
    "m2 = torch.nn.GRU(ic,hc,num_layers =2,batch_first=True)\n",
    "\n",
    "x = torch.rand(B,L,ic)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "N = 10\n",
    "\n",
    "x = x.to(\"cuda\")\n",
    "m1 = m1.to(\"cuda\")\n",
    "m2 = m2.to(\"cuda\")\n",
    "\n",
    "tic =  time.time()\n",
    "for i in tqdm(range(N)):\n",
    "    y = m1(x)[0]\n",
    "toc =  time.time()\n",
    "\n",
    "print((toc - tic)/N)\n",
    "\n",
    "tic =  time.time()\n",
    "for i in tqdm(range(N)):\n",
    "    y = m2(x)[0]    \n",
    "toc =  time.time()\n",
    "print((toc - tic)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9be27f-6e1e-45af-a2d3-9d80662c032a",
   "metadata": {},
   "source": [
    "# Skip RNN\n",
    "https://github.com/gitabcworld/skiprnn_pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b5b78a2-9afc-495d-8fad-8122ddbacd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class RNNCellBase(nn.Module):\n",
    "    __constants__ = ['input_size', 'hidden_size', 'bias']\n",
    "\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    bias: bool\n",
    "    # WARNING: bias_ih and bias_hh purposely not defined here.\n",
    "    # See https://github.com/pytorch/pytorch/issues/39670\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, bias: bool, num_chunks: int,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.weight_ih = nn.ParameterList([torch.empty((num_chunks * hidden_size, input_size), **factory_kwargs)])\n",
    "        self.weight_hh = nn.ParameterList([torch.empty((num_chunks * hidden_size, hidden_size), **factory_kwargs)])\n",
    "        if bias:\n",
    "            self.bias_ih = nn.ParameterList([torch.empty(num_chunks * hidden_size, **factory_kwargs)])\n",
    "            self.bias_hh = nn.ParameterList([torch.empty(num_chunks * hidden_size, **factory_kwargs)])\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if 'bias' in self.__dict__ and self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n",
    "            s += ', nonlinearity={nonlinearity}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size) if self.hidden_size > 0 else 0\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "\n",
    "class CCellBase(RNNCellBase):\n",
    "\n",
    "    def __init__(self, cell, learnable_elements, input_size, hidden_size, num_layers = 1, num_chunks=3,\n",
    "                    bias=True, batch_first = False, activation=F.tanh, layer_norm=False):\n",
    "        print(input_size)\n",
    "        print(hidden_size)\n",
    "        super(CCellBase, self).__init__(input_size, hidden_size, bias,num_chunks )\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.cell = cell\n",
    "        self.num_layers = num_layers\n",
    "        self.weight_ih = nn.ParameterList([])\n",
    "        self.weight_hh = nn.ParameterList([])\n",
    "        self.bias_ih = nn.ParameterList([])\n",
    "        self.bias_hh = nn.ParameterList([])\n",
    "\n",
    "        for i in np.arange(self.num_layers):\n",
    "            if i == 0:\n",
    "                weight_ih = Parameter(xavier_uniform_(torch.Tensor(learnable_elements * hidden_size, input_size)))\n",
    "            else:\n",
    "                weight_ih = Parameter(xavier_uniform_(torch.Tensor(learnable_elements * hidden_size, hidden_size)))\n",
    "            weight_hh = Parameter(xavier_uniform_(torch.Tensor(learnable_elements * hidden_size, hidden_size)))\n",
    "            self.weight_ih.append(weight_ih)\n",
    "            self.weight_hh.append(weight_hh)\n",
    "            if bias:\n",
    "                bias_ih = Parameter(torch.zeros(learnable_elements * hidden_size))\n",
    "                bias_hh = Parameter(torch.zeros(learnable_elements * hidden_size))\n",
    "                self.bias_ih.append(bias_ih)\n",
    "                self.bias_hh.append(bias_hh)\n",
    "            else:\n",
    "                self.register_parameter('bias_ih_' + str(i), None)\n",
    "                self.register_parameter('bias_hh_' + str(i), None)\n",
    "        self.weight_ih = nn.ParameterList(self.weight_ih)\n",
    "        self.weight_hh = nn.ParameterList(self.weight_hh)\n",
    "        if self.bias_ih:\n",
    "            self.bias_ih = nn.ParameterList(self.bias_ih)\n",
    "            self.bias_hh = nn.ParameterList(self.bias_hh)\n",
    "\n",
    "        self.activation = activation\n",
    "        self.layer_norm = layer_norm\n",
    "        self.lst_bnorm_rnn = None \n",
    "        \n",
    "class CCellBaseSkipGRU(CCellBase):\n",
    "\n",
    "    def __init__(self, cell, learnable_elements, input_size, hidden_size, num_layers = 1,\n",
    "                    bias=True, batch_first = False, activation=F.tanh, layer_norm=False):\n",
    "        super(CCellBaseSkipGRU, self).__init__(cell, learnable_elements, input_size, hidden_size, num_layers,\n",
    "                                               bias, batch_first, activation, layer_norm)\n",
    "        self.weight_uh = Parameter(xavier_uniform_(torch.Tensor(1, hidden_size)))\n",
    "        if bias:\n",
    "            self.bias_uh = Parameter(torch.ones(1))\n",
    "        else:\n",
    "            self.register_parameter('bias_uh', None)\n",
    "\n",
    "    def forward(self, input, hx = None):\n",
    "        if len(input.shape) == 3:\n",
    "            if self.batch_first:\n",
    "                input = input.transpose(0,1)\n",
    "            sequence_length, batch_size, input_size = input.shape\n",
    "        else:\n",
    "            sequence_length = 1\n",
    "            batch_size, input_size = input.shape\n",
    "\n",
    "        if hx is None:\n",
    "            hx = self.init_hidden(batch_size)\n",
    "            if input.is_cuda:\n",
    "                if self.num_layers == 1:\n",
    "                    hx = tuple([x.cuda() for x in hx])\n",
    "                else:\n",
    "                    hx = [tuple([j.cuda() if j is not None else None for j in i]) for i in hx]\n",
    "\n",
    "        \"\"\"  Deprecated ? \n",
    "        if len(input.shape) == 3:\n",
    "            self.check_forward_input(input[0])\n",
    "            if self.num_layers > 1:\n",
    "                self.check_forward_hidden(input[0], hx[0][0], '[0]')\n",
    "            lse:\n",
    "                self.check_forward_hidden(input[0], hx[0], '[0]')\n",
    "        else:\n",
    "            self.check_forward_input(input)\n",
    "            if self.num_layers > 1:\n",
    "                self.check_forward_hidden(input, hx[0][0], '[0]')\n",
    "            else:\n",
    "                self.check_forward_hidden(input, hx[0], '[0]')\n",
    "        \"\"\"\n",
    "    \n",
    "        # Initialize batchnorm layers\n",
    "        if self.layer_norm and self.lst_bnorm_rnn is None:\n",
    "            self.lst_bnorm_rnn = []\n",
    "            for i in np.arange(self.num_layers):\n",
    "                # Create gain and bias for input_gate, new_input, forget_gate, output_gate\n",
    "                lst_bnorm_rnn_tmp = torch.nn.ModuleList([nn.BatchNorm1d(self.hidden_size) for i in np.arange(2)])\n",
    "                if input.is_cuda:\n",
    "                    lst_bnorm_rnn_tmp = lst_bnorm_rnn_tmp.cuda()\n",
    "                self.lst_bnorm_rnn.append(lst_bnorm_rnn_tmp)\n",
    "            self.lst_bnorm_rnn = torch.nn.ModuleList(self.lst_bnorm_rnn)\n",
    "\n",
    "        lst_output = []\n",
    "        lst_update_gate = []\n",
    "        for t in np.arange(sequence_length):\n",
    "            output, hx = self.cell(\n",
    "                input[t], hx, self.num_layers,\n",
    "                self.weight_ih, self.weight_hh, self.weight_uh,\n",
    "                self.bias_ih, self.bias_hh, self.bias_uh,\n",
    "                activation=self.activation,\n",
    "                lst_layer_norm=self.lst_bnorm_rnn\n",
    "            )\n",
    "            new_h, update_gate = output\n",
    "            lst_output.append(new_h)\n",
    "            lst_update_gate.append(update_gate)\n",
    "        output = torch.stack(lst_output)\n",
    "        update_gate = torch.stack(lst_update_gate)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "            update_gate = update_gate.transpose(0, 1)\n",
    "        return output, hx, update_gate\n",
    "\n",
    "class BinaryLayer(Function):\n",
    "    def forward(self, input):\n",
    "        return input.round()\n",
    " \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output    \n",
    "    \n",
    "def MultiSkipGRUCell(input, state, num_layers, w_ih, w_hh, w_uh,b_ih=None, b_hh=None, b_uh=None,\n",
    "                  activation=F.tanh, lst_layer_norm=None):\n",
    "\n",
    "    _ , update_prob_prev, cum_update_prob_prev = state[-1]\n",
    "    cell_input = input\n",
    "    state_candidates = []\n",
    "\n",
    "    for idx in np.arange(num_layers):\n",
    "\n",
    "        h_prev, _, _ = state[idx]\n",
    "\n",
    "        gi = F.linear(cell_input, w_ih[idx], b_ih[idx])\n",
    "        gh = F.linear(h_prev, w_hh[idx], b_hh[idx])\n",
    "        i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "\n",
    "        resetgate_tmp = i_r + h_r\n",
    "        inputgate_tmp = i_i + h_i\n",
    "        if lst_layer_norm:\n",
    "            resetgate_tmp = lst_layer_norm[idx][0](resetgate_tmp.contiguous())\n",
    "            inputgate_tmp = lst_layer_norm[idx][1](inputgate_tmp.contiguous())\n",
    "\n",
    "        resetgate = F.sigmoid(resetgate_tmp)\n",
    "        inputgate = F.sigmoid(inputgate_tmp)\n",
    "\n",
    "        newgate = activation(i_n + resetgate * h_n)\n",
    "        new_h_tilde = newgate + inputgate * (h_prev - newgate)\n",
    "\n",
    "        state_candidates.append(new_h_tilde)\n",
    "        cell_input = new_h_tilde\n",
    "\n",
    "    # Compute value for the update prob\n",
    "    new_update_prob_tilde = F.sigmoid(F.linear(state_candidates[-1], w_uh, b_uh))\n",
    "\n",
    "    # Compute value for the update gate\n",
    "    cum_update_prob = cum_update_prob_prev + torch.min(update_prob_prev, 1. - cum_update_prob_prev)\n",
    "    # round\n",
    "    bn = BinaryLayer()\n",
    "    update_gate = bn(cum_update_prob)\n",
    "    # Apply update gate\n",
    "    new_states = []\n",
    "    for idx in np.arange(num_layers - 1):\n",
    "        new_h = update_gate * state_candidates[idx] + (1. - update_gate) * state[idx][0]\n",
    "        new_states.append((new_h,None,None))\n",
    "    new_h = update_gate * state_candidates[-1] + (1. - update_gate) * state[-1][0]\n",
    "\n",
    "    new_update_prob = update_gate * new_update_prob_tilde + (1. - update_gate) * update_prob_prev\n",
    "    new_cum_update_prob = update_gate * 0. + (1. - update_gate) * cum_update_prob\n",
    "    new_states.append((new_h, new_update_prob, new_cum_update_prob))\n",
    "    new_output = (new_h, update_gate)\n",
    "\n",
    "    return new_output, new_states    \n",
    "    \n",
    "    \n",
    "class CMultiSkipGRUCell(CCellBaseSkipGRU):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CMultiSkipGRUCell, self).__init__(cell=MultiSkipGRUCell, learnable_elements=3, *args, **kwargs)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        initial_states = []\n",
    "        for i in np.arange(self.num_layers):\n",
    "            initial_h = Variable(torch.randn(batch_size, self.hidden_size))\n",
    "            if i == self.num_layers - 1: #last layer\n",
    "                initial_update_prob = Variable(torch.ones(batch_size, 1),requires_grad=False)\n",
    "                initial_cum_update_prob = Variable(torch.zeros(batch_size, 1),requires_grad=False)\n",
    "            else:\n",
    "                initial_update_prob = None\n",
    "                initial_cum_update_prob = None\n",
    "            initial_states.append((initial_h,initial_update_prob,initial_cum_update_prob))\n",
    "        return initial_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0fd38-68d3-4a47-a811-f864b7d97309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "torch.Size([64, 40, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_3224396/2675326793.py\u001b[0m(203)\u001b[0;36mMultiSkipGRUCell\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    201 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    202 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 203 \u001b[0;31m        \u001b[0mnewgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresetgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mnew_h_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewgate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnewgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    205 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  F.tanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function tanh at 0x7f91ab5df700>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "ic = 256\n",
    "hc = 256\n",
    "B = 64\n",
    "L = 40\n",
    "\n",
    "m1 = CMultiSkipGRUCell(input_size=ic, hidden_size=hc, batch_first=True, num_layers=2)\n",
    "m2 = torch.nn.GRU(ic,hc,num_layers =2,batch_first=True)\n",
    "\n",
    "x = torch.rand(B,L,ic)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "N = 10\n",
    "\n",
    "x = x.to(\"cuda\")\n",
    "m1 = m1.to(\"cuda\")\n",
    "m2 = m2.to(\"cuda\")\n",
    "\n",
    "tic =  time.time()\n",
    "for i in tqdm(range(N)):\n",
    "    y = m1(x)[0]\n",
    "toc =  time.time()\n",
    "\n",
    "print((toc - tic)/N)\n",
    "\n",
    "tic =  time.time()\n",
    "for i in tqdm(range(N)):\n",
    "    y = m2(x)[0]    \n",
    "toc =  time.time()\n",
    "print((toc - tic)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cb571a1-9e27-4dfe-b8ce-58ae948c17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cellModule(nn.Module):\n",
    "\n",
    "    def __init__(self, cells, model):\n",
    "        super(cellModule, self).__init__()\n",
    "        self.model = model\n",
    "        self.rnn = cells\n",
    "        self.d1 = nn.Linear(FLAGS['rnn_cells'],OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        if hx is not None:\n",
    "            output = self.rnn(input, hx)\n",
    "        else:\n",
    "            output = self.rnn(input)\n",
    "        output, hx, updated_state = split_rnn_outputs(self.model, output)\n",
    "        output = self.d1(output[:,-1,:]) # Get the last output of the sequence\n",
    "        return output, hx, updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d08c99-2ad5-4a67-aaa4-26445566f09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
